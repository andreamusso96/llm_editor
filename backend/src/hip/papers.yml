- paper_id: 1
  title: Human mobility networks reveal increased segregation in large cities
  introduction: |
    In the United States, economic segregation is very high, with income affecting where one lives, who one marries, and who one meets and befriends. This extreme segregation is costly. It reduces economic mobility, fosters a wide range of health problems and increases political polarization. Although there are all manner of reforms designed to reduce economic segregation (such as subsidized housing), it has long been argued that one of the most powerful segregation-reducing dynamics is rising urbanization and the resulting happenstance mixing that it induces. This ‘cosmopolitan mixing hypothesis’ anticipates that, in large cities, the combination of increased population diversity, constrained space and accessible public transportation will bring diverse individuals into close physical proximity with one another, reducing everyday socioeconomic segregation. The New York City Subway has been lauded, for example, as a mixing bowl in which a diverse set of people cross paths each day.

    As plausible as the cosmopolitan mixing hypothesis might seem, big cities also provide new opportunities for self-segregation, because they are large enough to enable people to seek out and find others who are similar to themselves. These contrasting hypotheses about the relationship between urbanization and socioeconomic mixing remain untested because it has been difficult to measure real-world exposures that take the form of path crossings and encounters among individuals. It becomes possible to measure such exposures when mobile phone geolocation data are analysed at the device level. Although mobile phone data have been used for many research

    Here we carefully test the cosmopolitan mixing hypothesis and the dynamics underlying it. To assess this hypothesis and understand the relationship between urbanization and segregation, we use mobile phone mobility data in the form of de-identified GPS location pings (see the ‘SafeGraph’ section of the Methods). From this data, we capture geolocated individual-level exposures between individuals of similar or different SES. This enables us to develop city-level and county-level measures of segregation that capture where people go, when they go there and whom they encounter on the way.

    We first determine the SES of a person by identifying their home location and its monthly rent value. We next construct a dynamic network that captures each individual’s exposures to other individuals in their everyday life. Our network contains 1,570,782,460 edges (representing exposures in physical space) among 9,567,559 nodes (representing individuals, that is, mobile phones) across 382 MSAs and 2,829 counties in the United States. Every timestamped edge between a pair of nodes signifies that the two individuals crossed paths with and encountered each other (that is, they were at the same location at the same time). We analysed these data to estimate the amount of exposure segregation, defined as the extent to which individuals of different economic statuses are exposed to one another within each geographical area (MSAs and counties) in the United States. Our measure of exposure segregation extends a traditional static segregation measure by capturing the diversity of person-to-person exposures localized in space and time.

- paper_id: 2
  title: "The growth equation of cities"
  introduction: |
    Constructing a science of cities has become a crucial task for our societies, which are growing ever more concentrated in urban systems. Better planning could be achieved with a better understanding of city growth and how it affects society and the environment12. Various important aspects of cities such as urban sprawl, infrastructure development or transport planning depend on the population evolution over time, and multiple theoretical attempts have been made in order to understand this crucial phenomenon.

    So far, most research in city growth has been done with the idea that the stationary state for a set of cities is described by Zipf’s law. This law is considered to be a cornerstone of urban economics and geography, and states that the population distribution of urban areas in a given territory (or country) displays a Pareto law with exponent equal to 2 or, equivalently, that the city populations sorted in decreasing order versus their ranks follow a power law with exponent 1. This alleged regularity through time and space is probably the most striking fact in the science of cities and for more than a century has triggered intense debate and many studies. This result characterizes the hierarchical organization of cities, and in particular it quantifies the statistical occurrence of large cities. Zipf’s law implies that in any country, the city with the largest population is generally twice as large as the next largest, and so on. It is a signature of the very large heterogeneity of city sizes and shows that cities are not governed by optimal considerations that would lead to one unique size but, on the contrary, that city sizes are broadly distributed and follow some sort of hierarchy. The empirical value of the Pareto exponent informs us about the hierarchical degree of a system of cities: a large value of the exponent corresponds to a more equally distributed population among cities, and, vice versa, for

    Studies in economics have suggested that Zipf’s law is the result of economic shocks and random growth processes. Gabaix proved in a seminal paper that Gibrat’s law of random growth—which assumes a population growth rate independent of the size of the city—can lead to a Zipf law with exponent 1, at the expense of the additional and untested assumption that cities cannot become too small. This model remains the most accepted paradigm to understand city growth. Since then, it has also been understood using simplified theoretical models (without any empirical arguments) that migrations from other cities or countries are determinant in explaining random growth. However, although most of these theoretical approaches focus on explaining Zipf’s law with exponent 1, recent empirical studies, supported by an increasing number of data sources, have questioned the existence of such a universal power law and have shown that Zipf’s exponent can vary around 1 depending on the country, the time period, the definition of cities used or the fitting method (we illustrate this in Extended Data Fig. 1, showing that no universal result for the population distribution is observed), leading to the idea that there is no reason to think that Zipf’s law holds in all cases.

    Beyond understanding the stationary distribution of urban populations lies the problem of their temporal evolution. As already noted5, the huge number of studies regarding population distribution contrasts with the few analyses of the time evolution of cities. As discussed in that same work5, cities and civilizations rise and fall many times on a large range of time scales, and Gabaix’s model is both quantitatively and qualitatively unable to explain these specific chaotic dynamics.

    Therefore, a model able to simultaneously explain observations about the stationary population distribution and the temporal dynamics of systems of cities is missing. In particular, we are not at this point able to identify the causes of the diversity of empirical observations about the hierarchical organization of cities, the occurrence of megacities, and the empirical instability in city dynamics seen in the births and deaths of large cities on short time scales. In this respect, we do not need just a quantitative improvement of models but a shift of paradigm.

    In this paper, we show that city growth is dominated by rare events—namely large interurban migratory shocks—rather than by the average growth rate. Rare but large positive or negative migratory flows can destabilize the hierarchy and the dynamics of a city on very short time scales, leading to the disordered dynamics of cities observed throughout history. On the basis of an empirical analysis of migrations flows in four countries, in the following we derive a stochastic equation of city growth that is able to explain empirical observations of the statistics and temporal dynamics of cities.


- paper_id: 3
  title: "Urban scaling laws arise from within-city inequalities"
  introduction: |
    In recent years, researchers from across disciplines have identified striking and seemingly universal relationships between city size and various urban quantities. Cities’ total outputs increase more than proportionately with increases in city size, suggesting that inhabitants of larger cities are, on average, better off economically. This relationship has been described by a power-law function of the form Y~Y0Nβ, where Y represents a city-aggregated socio-economic quantity, N is population size, Y0 is a normalization constant and β is a scaling exponent capturing the non-linear change in Y as a function of N. Estimates of β > 1 indicate greater socio-economic output per capita with increasing city size.

    To explain such superlinear scaling relations, reference has been made to increasing levels of social interconnectivity in dense urban environments. This interpretation meets earlier descriptions of cities as ecosystems of social exchange and, remarkably, simple formalizations of cities as interconnected networks provide predictions that map very well onto empirical observations of superlinear scaling. More recent research has added economic complementarities and the higher industrial complexity found in larger cities to the list of crucial drivers of urban scaling phenomena.

    However, the main tenet of the scaling paradigm assumes strong levels of homogeneity. It assumes that the residents of a city have roughly equal numbers of network contacts and that the companies in a specific urban industry have similar levels of economic complexity, and thus—as implied by the theory—have approximately equal levels of productivity. Empirical studies have built on this assumption in their use of city sums and means to capture agglomeration effects, as well as in their interpretations, which focus on the ‘average’ resident or firm. Prior research therefore implicitly painted a picture in which scaling effects are driven by a homogeneous shift of the whole city distribution as the population grows larger (see Discussion for further elaboration and Supplementary Note 6 in the Supplementary Information for a detailed review of a recent mathematical framework that represents the state of the art).

    The homogeneity assumption is attractive because it renders mathematical models tractable and empirical analyses straightforward. But—as literature from both the social sciences and complexity research has documented—human networking and productivity show heavy-tailed distributions in which small fractions of extremely well-connected or highly successful individuals contribute large proportions to city totals. Power laws are common in nature and society, present not only as scaling laws between cities but also as extremely skewed distributions within agglomerations. Consequently, sums and means are poor and potentially misleading indicators of the relevant quantities of cities.

    Acknowledging the extreme skewness of urban indicators implies a discrepancy between observed distributions and the assumptions made by theoretical models, and it suggests an inadequacy of the measures used to test their predictions. A number of questions that go beyond the scope of current theoretical models naturally follow from this. Does within-city tailedness—which we define as the relative contribution of the top ten percentiles in a city, and which therefore also reflects urban inequality—differ systematically by city size? If so, how much of the previously reported superlinear scaling can be attributed to differences in cities’ tails, defined here as individuals or firms in the top (≥90th) percentiles of within-city distributions, as opposed to differences in their mass, which we define as being represented by the typical resident or firm (50th percentile) in a given city. Do phenomena that have heavier tails scale more or less than those with smaller tails? If they do, how much of the variance in scaling exponents across complexity categories (for example, occupations and industries) can be explained by differences in the tails? And, importantly, if within-city tails turn out to be essential to between-city scaling, what mechanisms underlie the emergence of tail differences by city size? These are the questions that we aim to answer in this article.

    We use micro-level data from Sweden, Russia and the United States that provide detailed information of within-city distributions of interconnectivity, productivity and innovation. First, we call attention to urban indicators’ heavy tails, particularly in larger cities. Second, we quantify the implications that differences in city tails have for urban scaling. Our findings show that cities’ tails—and, crucially, their growth as cities become larger—disproportionately contribute to superlinear scaling between cities. While we obtain scaling coefficients for city means that are in line with prior results, we find that cities’ tails are responsible for 36–80% of the observed superlinearities across indicators. Additionally, we find that tails explain most of the differences in scaling coefficients between indicators of various levels of complexity. This implies, for example, that once within-city tails have been discarded, average productivity differences by city size are similar for starkly different sectors, such as information technology firms and restaurants. Third, we provide a formal description—in the form of a computational model—of the positive link between the size of tails of within-city distributions and scaling exponents. The model marks out the conditions that give rise to a city size-dependent cumulative advantage mechanism, according to which large cities provide for some people novel opportunities for sustained growth, and it shows how tail differences by city size are brought about at the macro level. The model reproduces our main results, and micro-level data on the earnings trajectories of 1.4 million Swedes confirm the model’s prediction of greater cumulative advantage effects for tail units in larger cities, and thus of their disproportional contribution to superlinear scaling.

    These results have ramifications for the dominant mean-field interpretation of urban scaling. In revealing the crucial role of within-city tails, our findings point towards a different understanding of between-city scaling, where agglomeration effects operate on and intensify urban inequality. Our research implies that the causal processes underlying heavier tails in larger cities constitute an indispensable element of urban scaling, and that any theory seeking to explain urban scaling—whether it be through interconnectivity, complexity or other factors—must also explain the emergence of tail differences by city size.

- paper_id: 4
  title: "Persistent interaction patterns across social media platforms and over time"
  introduction: |
    The advent and proliferation of social media platforms have not only transformed the landscape of online participation but have also become integral to our daily lives, serving as primary sources for information, entertainment and personal communication. Although these platforms offer unprecedented connectivity and information exchange opportunities, they also present challenges by entangling their business models with complex social dynamics, raising substantial concerns about their broader impact on society. Previous research has extensively addressed issues such as polarization, misinformation and antisocial behaviours in online spaces, revealing the multifaceted nature of social media’s influence on public discourse. However, a considerable challenge in understanding how these platforms might influence inherent human behaviours lies in the general lack of accessible data. Even when researchers obtain data through special agreements with companies like Meta, it may not be enough to clearly distinguish between inherent human behaviours and the effects of the platform’s design. This difficulty arises because the data, deeply embedded in platform interactions, complicate separating intrinsic human behaviour from the influences exerted by the platform’s design and algorithms.

    Here we address this challenge by focusing on toxicity, one of the most prominent aspects of concern in online conversations. We use a comparative analysis to uncover consistent patterns across diverse social media platforms and timeframes, aiming to shed light on toxicity dynamics across various digital environments. In particular, our goal is to gain insights into inherently invariant human patterns of online conversations.

    The lack of non-verbal cues and physical presence on the web can contribute to increased incivility in online discussions compared with face-to-face interactions. This trend is especially pronounced in online arenas such as newspaper comment sections and political discussions, where exchanges may degenerate into offensive comments or mockery, undermining the potential for productive and democratic debate. When exposed to such uncivil language, users are more likely to interpret these messages as hostile, influencing their judgement and leading them to form opinions based on their beliefs rather than the information presented and may foster polarized perspectives, especially among groups with differing values. Indeed, there is a natural tendency for online users to seek out and align with information that echoes their pre-existing beliefs, often ignoring contrasting views. This behaviour may result in the creation of echo chambers, in which like-minded individuals congregate and mutually reinforce shared narratives5,24,25. These echo chambers, along with increased polarization, vary in their prevalence and intensity across different social media platforms1, suggesting that the design and algorithms of these platforms, intended to maximize user engagement, can substantially shape online social dynamics. This focus on engagement can inadvertently highlight certain behaviours, making it challenging to differentiate between organic user interaction and the influence of the platform’s design. A substantial portion of current research is devoted to examining harmful language on social media and its wider effects, online and offline. This examination is crucial, as it reveals how social media may reflect and amplify societal issues, including the deterioration of public discourse. The growing interest in analysing online toxicity through massive data analysis coincides with advancements in machine learning capable of detecting toxic language27. Although numerous studies have focused on online toxicity, most concentrate on specific platforms and topics28,29. Broader, multiplatform studies are still limited in scale and reach. Research fragmentation complicates understanding whether perceptions about online toxicity are accurate or misconceptions31. Key questions include whether online discussions are inherently toxic and how toxic and non-toxic conversations differ. Clarifying these dynamics and how they have evolved over time is crucial for developing effective strategies and policies to mitigate online toxicity.

    Our study involves a comparative analysis of online conversations, focusing on three dimensions: time, platform and topic. We examine conversations from eight different platforms, totalling about 500 million comments. For our analysis, we adopt the toxicity definition provided by the Perspective API, a state-of-the-art classifier for the automatic detection of toxic speech. This API considers toxicity as “a rude, disrespectful or unreasonable comment likely to make someone leave a discussion”. We further validate this definition by confirming its consistency with outcomes from other detection tools, ensuring the reliability and comparability of our results. The concept of toxicity in online discourse varies widely in the literature, reflecting its complexity, as seen in various studies. The efficacy and constraints of current machine-learning-based automated toxicity detection systems have recently been debated. Despite these discussions, automated systems are still the most practical means for large-scale analyses.

    Here we analyse online conversations, challenging common assumptions about their dynamics. Our findings reveal consistent patterns across various platforms and different times, such as the heavy-tailed nature of engagement dynamics, a decrease in user participation and an increase in toxic speech in lengthier conversations. Our analysis indicates that, although toxicity and user participation in debates are independent variables, the diversity of opinions and sentiments among users may have a substantial role in escalating conversation toxicity.

    To obtain a comprehensive picture of online social media conversations, we analysed a dataset of about 500 million comments from Facebook, Gab, Reddit, Telegram, Twitter, Usenet, Voat and YouTube, covering diverse topics and spanning over three decades (a dataset breakdown is shown in Table 1 and Supplementary Table 1; for details regarding the data collection, see the ‘Data collection’ section of the Methods).

    Our analysis aims to comprehensively compare the dynamics of diverse social media accounting for human behaviours and how they evolved. In particular, we first characterize conversations at a macroscopic level by means of their engagement and participation, and we then analyse the toxicity of conversations both after and during their unfolding. We conclude the paper by examining potential drivers for the emergence of toxic speech.

- paper_id: 5
  title: "Papers and patents are becoming less disruptive over time"
  introduction: |
    Although the past century witnessed an unprecedented expansion of scientific and technological knowledge, there are concerns that innovative activity is slowing. Studies document declining research productivity in semiconductors, pharmaceuticals and other fields. Papers, patents and even grant applications have become less novel relative to prior work and less likely to connect disparate areas of knowledge, both of which are precursors of innovation. The gap between the year of discovery and the awarding of a Nobel Prize has also increased, suggesting that today’s contributions do not measure up to the past. These trends have attracted increasing attention from policymakers, as they pose substantial threats to economic growth, human health and wellbeing, and national security, along with global efforts to combat grand challenges such as climate change.

    Numerous explanations for this slowdown have been proposed. Some point to a dearth of ‘low-hanging fruit’ as the readily available productivity-enhancing innovations have already been made. Others emphasize the increasing burden of knowledge; scientists and inventors require ever more training to reach the frontiers of their fields, leaving less time to push those frontiers forward. Yet much remains unknown, not merely about the causes of slowing innovative activity, but also the depth and breadth of the phenomenon. The decline is difficult to reconcile with centuries of observation by philosophers of science, who characterize the growth of knowledge as an endogenous process, wherein previous knowledge enables future discovery, a view captured famously in Newton’s observation that if he had seen further, it was by ‘standing on the shoulders of giants’. Moreover, to date, the evidence pointing to a slowdown is based on studies of particular fields, using disparate and domain-specific metrics, making it difficult to know whether the changes are happening at similar rates across areas of science and technology. Little is also known about whether the patterns seen in aggregate indicators mask differences in the degree to which individual works push the frontier.

    We address these gaps in understanding by analysing 25 million papers (1945–2010) in the Web of Science (WoS) (Methods) and 3.9 million patents (1976–2010) in the United States Patent and Trademark Office’s (USPTO) Patents View database (Methods). The WoS data include 390 million citations, 25 million paper titles and 13 million abstracts. The Patents View data include 35 million citations, 3.9 million patent titles and 3.9 million abstracts. Subsequently, we replicate our core findings on four additional datasets—JSTOR, the American Physical Society corpus, Microsoft Academic Graph and PubMed—encompassing 20 million papers. Using these data, we join a new citation-based measure12 with textual analyses of titles and abstracts to understand whether papers and patents forge new directions over time and across fields.

- paper_id: 6
  title: "The rise of the super-rich"
  introduction: |
    Advocates of team science have claimed that a shift to larger teams in science and technology fulfils the essential function of solving problems in modern society that are complex and which require interdisciplinary solutions. Although much has been demonstrated about the professional and career benefits of team size for team members, there is little evidence that supports the notion that larger teams are optimized for knowledge discovery and technological invention. Experimental and observational research on groups reveals that individuals in large groups think and act differently—they generate fewer ideas, recall less learned information, reject external perspectives more often and tend to neutralize each other’s viewpoints. Small and large teams may also differ in their response to the risks associated with innovation. Large teams, such as large business organizations, may focus on sure bets with large potential markets, whereas small teams that have more to gain and less to lose may undertake new, untested opportunities with the potential for high growth and failure, leading to markedly different outcomes. These possibilities led us to explore the consequences of smaller and larger teams for scientific and technological advance, and how such teams search and assemble knowledge differently.

    Previous research demonstrates that large article and patent teams receive slightly more citations. However, citation counts alone cannot capture distinct types of contribution. This can be seen in the difference between two well-known articles: one about self-organized criticality (the BTW model, after the authors’ initials) and another about Bose–Einstein condensation (for which Wolfgang Ketterle was awarded the 2001 Nobel Prize in Physics). The two articles have received a similar number of citations, but most research subsequent to the BTW-model article has cited only the model itself without mentioning references from the article. By contrast, the Bose–Einstein condensation article is almost always co-cited with Bose, Einstein and other antecedents. The difference between the two papers is reflected not in citation counts but in whether they suggested or solved scientific problems—whether they disrupted or developed existing scientific ideas, respectively. The BTW model launched new streams of research, whereas the experimental realization of Bose–Einstein condensation elaborated upon possibilities that had previously been posed.

    To systematically evaluate the role that small and large teams have in unfolding scientific and technological advances, we collected large-scale datasets from three related but distinct domains (see Methods): (1) the Web of Science (WOS) database that contains more than 42 million articles published between 1954 and 2014, and 611 million citations among them; (2) 5 million patents granted by the US Patent and Trademark Office from 1976 to 2014, and 65 million citations added by patent applicants; (3) 16 million software projects and 9 million forks to them on GitHub (2011–2014), a popular web platform that allows users to collaborate on the same code repository and ‘cite’ other repositories by copying and building on their code.

- paper_id: 7
  title: "Online searches to evaluate misinformation can increase its perceived veracity"
  introduction: |
    Concern over the impact of misinformation has continued to grow, as high levels of belief in misinformation have threatened democratic legitimacy in the United States and global public health during the COVID-19 pandemic. Considerable attention among scholars, media and policymakers alike has been paid to the role of social media platforms in the spread of, and belief in, misinformation, with comparatively little focus on other central features of the digital information ecosystem.

    This gap in research is particularly evident in our limited understanding of the effect of search engines. Although recent research has explored the potential partisan biases of search engine results, relatively little is known about the fundamental but understudied question of how searching online to evaluate news (SOTEN) impacts belief in misinformation. As the cost of producing and distributing information online has fallen and the sheer volume of information on the internet has risen, reliance on traditional gatekeepers has been substantially reduced, leaving search engines to fill the role of twenty-first-century gatekeepers by sorting and validating online content for the public. In this new role, search engines have become influential in users’ political knowledge10 and public opinion. A majority of internet users state that they check facts online that they come across at least once a day, and many believe that results from search engines are more reliable than traditional news, such as radio, newspapers or television11. The growing reliance on search engines for information verification has been encouraged by social media companies, civil society organizations13 and government agencies, all of which have invested in campaigns to encourage online users to research news they believe may be suspect through online search engines with the goal of reducing belief in misinformation. Although search engines have a key role in how people evaluate information online, we know little about how SOTEN impacts belief in misinformation.

    Research on interventions designed to mitigate belief in misinformation has developed in recent years, but work has thus far focused on ideological congruence, psychological factors and digital media literacy. Here we present the results from experimental studies identifying how SOTEN affects belief in misinformation. Specifically, we test a preregistered hypothesis that searching online to assess the veracity of false or misleading articles increases the belief that these stories are true, contradicting what we believe to be the received wisdom underlying many search-based recommendations. We then examine a possible mechanism for why belief in false/misleading articles is increased by searching online to evaluate these articles: exposure to unreliable information. Although it is plausible that searching online may lead respondents to reputable sources contradicting the false article’s central claim, previous studies on information systems have suggested that there are topics or terms for which there exists unreliable information available to be returned by search engines. As a number of digital literacy guides focus specifically on identifying misinformation, our main analyses are limited to the effect of search on belief in misinformation; however, given that the average online media diet comprises substantially more true than false news, we also test a preregistered hypothesis that searching online to assess the veracity of true articles increases belief in those articles.

    To this end, we run five separate experiments that measure the effect of SOTEN on belief in popular false and true news stories for the point in time investigated. Four of these studies use survey experiments; the fifth combines survey and digital trace data of search results collected using a custom web browser plug-in. In each study, the individuals in both the control and treatment groups were asked to assess the veracity of news articles, but those in the treatment group were encouraged to search online for information (instructions to search online were provided by a partner organization and are provided in the Methods) to help with this assessment. In an additional experiment, explained in Supplementary Information O, we tested whether the effect of SOTEN was robust to changing the wording of these instructions and found similar effects (Extended Data Fig. 1). For all five studies, we used a pipeline (which was also preregistered) to select popular articles from both main-stream and non-mainstream media sources and then distribute them to respondents and professional fact-checkers (a full explanation of this process is provided in the Methods). A key feature of our design is the ability to collect real-time evaluations in the time period during which past research has shown that misinformation is most likely to be consumed.

    Taken together, the five studies provide consistent evidence that SOTEN increased belief in misinformation during the point in time investigated. In our fifth study, which tested explanations for the mechanism underlying this effect, we found evidence suggesting that exposure to lower-quality information in search results is associated with a higher probability of believing misinformation, but exposure to high-quality information is not. Moreover, we found that there is a search effect on belief in true news that is similar to the search effect on belief in false/misleading news: searching can make study participants more likely to believe that true news stories are true. However, when we subset the results by the quality of source, we found that, although online search can increase belief that true news from low-quality sources is true, there is no consistent effect in either direction on believing true news from mainstream sources is true.

- paper_id: 8
  title: "Online images amplify gender bias"
  introduction: |
    Images increasingly pervade the information we consume and communicate daily. The number of images in online search engines has leapt from thousands to billions in just two decades2. Every day, millions of people view and download images from platforms such as Google and Wikipedia, and millions more are socializing through hyper-visual platforms such as Instagram, Snapchat and TikTok, which are based predominantly on the exchange of images. This growing trend is widely recognized by the tech and venture capital industries, as well as by news agencies and advertisers who are now relying more heavily on images to attract people’s attention online. This trend is also reflected by changes in the habits of the average American. A longitudinal survey from the American Academy of the Arts and Sciences shows that the amount of time Americans spend reading text is steadily declining1, whereas the time they spend producing and viewing images continues to rise2,4. What consequences does this unprecedented shift towards visual content have on how we ‘see’ the world? At the dawn of photography, Frederick Douglass—esteemed writer and civil rights leader—forewarned of the potential for images to reinforce social biases at large, arguing in his 1861 lecture ‘Pictures and Progress’ that “the great cheapness and universality of pictures must exert a powerful though silent influence on the ideas and sentiment of present and future generations”. Since Douglass’ time, the internet has made it only cheaper and easier to circulate images on a massive scale, potentially intensifying the impact of their silent influence. In this study, we explore the impact of online images on the large-scale spread of gender bias.

    Despite the swelling proliferation of online images, most quantitative research into online gender bias focuses on text. Only a few recent studies examine gender bias in a small sample of Google images16,17,18, without comparing the prevalence of gender bias and its psychological impact across images and text. Yet numerous psychological studies suggest that images may provide an especially potent medium for the transmission of gender bias. Research into the ‘picture superiority effect’ shows that images are often more memorable and emotionally evocative than text, and may implicitly underlie the comprehension of text itself. Images also differ from text in the salience with which they present demographic information. A textual description of a person can easily minimize gender bias by leveraging gender-neutral terminology or by omitting references to gender. For example, the sentence ‘The doctor administered the test’ makes no mention of the doctor’s gender. By contrast, an image of a doctor directly transmits demographic cues that elicit perceptions of the doctor’s gender. In this way, images strengthen the salience of gender in the representation of social categories. These intrinsic differences between images and text point to the prediction that online images amplify gender bias, both in its statistical prevalence and in its psychological impact on internet users.

- paper_id: 9
  title: Quantifying social organization and political polarization in online platforms
  introduction: |
    In 1962, Marshall McLuhan proclaimed that “The new electronic interdependence recreates the world in the image of a global village”5. In the decades since, there has been fierce debate about the internet’s dual forces of social integration, as the world becomes increasingly interconnected, and social fragmentation, as people can more easily select to join like-minded communities. Twenty years into the widespread adoption of online social media platforms, it remains unclear how online communities are socially organized. Of particular concern is whether online populations increasingly sort into homogeneous ‘echo chambers’ and whether social media platforms tend to shift users towards ideological extremes. However, since these platforms consist of massive amounts of unstructured and pseudonymous data, empirically quantifying the social makeup of online communities and, in turn, the social organization of online platforms, poses a unique challenge.

    Here we develop and validate a methodology using neural community embeddings9, which represent similarities in community membership as relationships between vectors in a high-dimensional space, to quantify the positioning of online communities along social dimensions. Focusing on traditional notions of identity—age, gender and political orientation—and leveraging the complete set of 5.1 billion comments made in 10,000 communities over a 14-year period on Reddit, one of the world’s largest social platforms, we produce an accurate and high-resolution picture of how the platform’s macroscale structure is organized along social lines. We then apply our methodology to quantify the dynamics and mechanisms of political polarization on Reddit, and investigate three related questions: (1) To what extent does platform-level political polarization change over time? (2) Do individual users become more polarized in their political activity over time, and if so, do these changes drive platform-level polarization? and (3) Are the dynamics of polarization ideologically symmetric?

    Our approach differs from prior work examining social organization and political polarization in online platforms in three main ways. First, our methodology avoids the biases that result from using self-reported data, expert labels and survey-based methods by quantifying the social makeup of communities in a purely behavioural fashion. Communities are similar only if their user bases are similar; by computing this similarity along a social dimension (for example, US political partisanship), we can recover an accurate estimate of whether a particular community’s user base is more behaviourally aligned with the left or right end of the spectrum (for example, the left or right wing of US politics). Users ‘vote with their feet’ to decide the social orientation of communities: only action across large numbers of people matters. Previous work has used word embeddings—high-dimensional representations of text—to study cultural stereotypes and the cultural markers of class13. Although our dataset comprises billions of comments, we do not use the text in our methodology. Differences in identity are reflected in the words people use, but this relationship is relatively weak for our focus on measuring the social orientation of underlying community populations. Communities that use similar language may be socially distinct, and communities with distinct language may be socially similar.

    Second, previous analyses have studied platforms such as Facebook, Twitter and Amazon, on which users are guided by algorithmic curation and personalized recommendations. Traces of user activity on these platforms reflect not only natural human choices but also the influence of algorithms. A recent focus has been on examining the effects of algorithmic curation on shaping online social organization—for example, measuring the prevalence of algorithmic ‘filter bubbles’ of homogeneous content and groups16,17—but user choices may have an even larger role in shaping this structure. Thus, although our methodology is generally applicable to many online platforms, we apply it here to Reddit, which has maintained a minimalist approach to personalized algorithmic recommendation throughout its history. The patterns of community memberships we observe are thus more likely to be reflective of the social organization induced by natural online behaviour.

    Finally, we expand the study of political polarization in social media. Polarization is understood as both a state and a process, but existing empirical research is largely limited to static analyses of incomplete and non-representative snapshots of activity on a platform. As such, although there is evidence that online platforms exist in states of partisan fragmentation, important questions about the dynamics and mechanisms of polarization processes remain unanswered. In particular, the measurement of platform-level polarization with incomplete and non-representative datasets is difficult, and tracking it over time with static analyses is impossible. Furthermore, any observed platform-level polarization could be due to two separate mechanisms with different policy implications: individual users could move towards ideological extremes in their activity over time, or relatively moderate populations could be replaced by new, more extreme populations as the user base turns over. Applying our methodology, we conduct dynamic analyses of complete platform activity to measure both platform- and individual-level polarization, and compare these for the left and right wings, over the entire history of Reddit.
  